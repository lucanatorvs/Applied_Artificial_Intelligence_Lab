# Lecture 3

22-11-23

## What are the different stages of a machine learning project (CRISPDM)?

- Business understanding
- Data understanding
- Data preparation
- Modeling
- Evaluation
- Deployment

see [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)

## What is the difference between supervised learning and unsupervised learning?

Supervised learning is when you have a dataset with the correct answers and you want to train a model to predict the correct answers. Unsupervised learning is when you have a dataset without the correct answers and you want to train a model to find patterns in the data.

## How does linear regression work for a single variable?

Linear regression is a type of supervised learning where the output is a continuous value. For a single variable, it is a one dimensional line. The goal is to find the line that minimizes the loss function.

## How does linear regression work for multiple variables?

For multiple variables, it is a multidimensional plane. The goal is to find the plane that minimizes the loss function.

## What is the “Normal Equation”?

The normal equation is a formula that gives the exact solution to the linear regression problem. It is given by: $\theta = (X^TX)^{-1}X^Ty$
Where $\theta$ is the vector of parameters, $X$ is the matrix of features, $y$ is the vector of outputs. $X^TX$ is the matrix product of $X$ and its transpose. $X^Ty$ is the matrix product of $X$ and $y$.

## What are batch, mini-batch and stochastic gradient descent?

Batch gradient descent is the method of updating the parameters by computing the gradient of the loss function with respect to the parameters using the entire training set.

Mini-batch gradient descent is the method of updating the parameters by computing the gradient of the loss function with respect to the parameters using a subset of the training set.

Stochastic gradient descent is the method of updating the parameters by computing the gradient of the loss function with respect to the parameters using a single training example.

## When and why is feature scaling necessary

Feature scaling is necessary when the features have different scales. For example, if one feature is in the range [0, 1] and another feature is in the range [0, 1000], then the gradient descent algorithm will take a long time to converge. Feature scaling is done by subtracting the mean and dividing by the standard deviation. Or by subtracting the minimum and dividing by the range.

## What is polynomial regression?

Polynomial regression is a type of linear regression where the features are transformed into higher order polynomials. For example, if the features are $x_1$ and $x_2$, then the features are transformed into $x_1^2$, $x_2^2$, $x_1x_2$, $x_1$ and $x_2$. This is done by adding higher order polynomials to the features, and then training a linear regression model on the new features.

## How can overfitting occur and what can we do to prevent it?

overfitting can occur when we add to manny higher order polynomials. U can detect this during the testiong phase.

## How does binary logistic regression work?

Binary logistic regression is a type of supervised learning where the output is a binary value. The goal is to find the line that minimizes the loss function. The loss function is the cross entropy loss function. The sigmoid function is used to transform the output to a value between 0 and 1.

## How does logistic regression work for multiple classes?

For multiple classes, the output is a vector of probabilities. The goal is to find the plane that minimizes the loss function. The loss function is the cross entropy loss function. The softmax function is used to transform the output to a vector of probabilities.

## What are sigmoid and softmax functions?

The sigmoid function is a function that maps the real numbers to the range [0, 1]. It is given by: $$\sigma(x) = \frac{1}{1 + e^{-x}}$$

The softmax function is a function that maps the real numbers to the range [0, 1] and sums to 1. It is given by: $$\sigma(x) = \frac{e^x}{\sum_{i=1}^n e^x_i}$$

## What is the loss function for logistic regression?

The loss function for logistic regression is the cross entropy loss function. It is given by: $$L = -\frac{1}{m}\sum_{i=1}^m y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)$$

## What is the cross entropy loss function?

The cross entropy loss function is a loss function that is used for classification problems. It is given by: $$L = -\frac{1}{m}\sum_{i=1}^m y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)$$
