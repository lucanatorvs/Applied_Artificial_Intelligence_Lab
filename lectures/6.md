# Lecture 6

22-12-21

## dimensionality reduction

- reduce the number of features in your dataset

## Approaches to dimensionality reduction

- project the data onto a lower dimensional subspace
  - e.g. PCA
    - try to find the subspace that preserves the most variance
- manifold learning
  - a manifold is a higher dimensional object that can be locally approximated by a lower dimensional object
    - e.g. a 2D manifold can be approximated by a line
    - e.g. a 3D manifold can be approximated by a plane

## pca with images

a image of 64x64 pixels can be represented as a 4096 dimensional vector
we project this vector onto a lower dimensional subspace

### howmuch data is retained?

the original data has some variance, and the projected data has some variance. the ratio of the two is the amount of data retained. it uses the variance of the original data as a measure of how much information is contained in the original data.

### how to find the subspace?

we want to find the subspace that preserves the most variance. we can do this by finding the eigenvectors of the covariance matrix of the original data. the eigenvectors with the highest eigenvalues are the ones that preserve the most variance.

### noise reduction

we can use pca to reduce the noise in an image. we can project the image onto a lower dimensional subspace, and then reconstruct the image from the projected data. the reconstructed image will have less noise than the original image.

## pca for visualization

we can use pca to visualize high dimensional data. we can project the data onto a 2D subspace, and then plot the data in the 2D subspace.

### we can also use TSNE

TSNE is a non-linear dimensionality reduction algorithm. it is useful for visualizing high dimensional data.

```python
from sklearn.manifold import TSNE
```

it applies pca to the data, and then applies a non-linear transformation to the data.

## PCA applications

- noise reduction
- compression
- anomaly detection
  - detect outliers
    - monitoring systems

## clustering

unsupervised learning

## k-means

randomly initialize k centroids. assign each data point to the nearest centroid. recompute the centroids. repeat until the centroids don't change.

(slice 19)

it is not guaranteed to converge to the global optimum. it is guaranteed to converge to a local optimum.

### choosing k

- elbow method
  - plot the sum of squared distances for different values of k
  - the elbow is the value of k that minimizes the sum of squared distances
    - the sum of the squared distance is the distance between a data point and the centroid of the cluster that the data point is assigned to

in the book thay takl about more sophisticated methods for choosing k, like silhouette analysis.

### what do you use k-means for?

- custom segmentation
  - eg. customer segmentation
- data analysis
  - eg. anomaly detection
- enevything else
  - eg. ;)

### k-means is not the only clustering algorithm

In this lecture, the concept of dimensionality reduction was introduced as a way to reduce the number of features in a dataset. Two approaches to dimensionality reduction were discussed: projecting the data onto a lower dimensional subspace and manifold learning, in which a higher dimensional object is locally approximated by a lower dimensional object. The specific technique of principal component analysis (PCA) was discussed in relation to images and its uses for noise reduction, compression, and anomaly detection. The topic of clustering, a form of unsupervised learning, was also introduced, specifically the k-means algorithm and methods for choosing the appropriate number of clusters. Other clustering algorithms, including hierarchical clustering, density-based clustering, mixture models, and spectral clustering, were also mentioned.

---

THATS IT FOR THIS LECTURE

## ware do we go from here?

- object detection
- reinforcement learning
- transformer models / self attention
  - state of the art for NLP
- autoencoders / gan's / defusion models

## What is the general goal of dimensionality reduction algorithms?

the goal of dimensionality reduction algorithms is to reduce the number of features in a dataset while retaining as much information as possible.

## What is principal component analysis and what is it used for?

analysis that finds the subspace that preserves the most variance in the data. it is used for noise reduction, compression, and anomaly detection.

## What is a principal component?  How is it related to the variance in the data?

??

## What does the 'explained_variance_ratio' tell us, and how do we use it?

the explained variance ratio tells us how much of the variance in the original data is preserved in the projected data. we can use it to determine how much data is retained when we project the data onto a lower dimensional subspace.

## What are some unsupervised learning techniques? and applications?

unsupervised learning techniques include clustering and dimensionality reduction. clustering is used for customer segmentation, anomaly detection, and everything else. dimensionality reduction is used for noise reduction, compression, and anomaly detection.

## What is K-means, how does it work and what are some applications?

K-means is a clustering algorithm. it randomly initializes k centroids, and then assigns each data point to the nearest centroid. it then recomputes the centroids, and repeats until the centroids don't change. it is used for customer segmentation, anomaly detection, and everything else.

## How can you choose the number of clusters?

you can use the elbow method, which plots the sum of squared distances for different values of k, and then chooses the value of k that minimizes the sum of squared distances.

## What are some problems/limitations with K-means?

it is not guaranteed to converge to the global optimum. it is guaranteed to converge to a local optimum.

## What is DBSCAN and how is it different from K-means?

?

---

## Summary

There are several applications of principal component analysis (PCA) in machine learning. Some of the main applications are:

- Data visualization: PCA can be used to reduce the dimensionality of high-dimensional data and visualize it in a lower-dimensional space. This can be useful for understanding patterns and relationships in the data.
- Noise reduction: PCA can be used to remove noise from a dataset by projecting the data onto a lower-dimensional subspace that preserves the most variance.
- Data compression: PCA can be used to compress a dataset by projecting it onto a lower-dimensional subspace and storing only the principal components. This can be useful for reducing storage and computational requirements.
- Anomaly detection: PCA can be used to identify anomalies or outliers in a dataset by analyzing the reconstruction error of the data after projecting it onto a lower-dimensional subspace.
- Feature selection: PCA can be used to select the most important features in a dataset by selecting the principal components that preserve the most variance. This can be useful for improving the performance of machine learning algorithms.
