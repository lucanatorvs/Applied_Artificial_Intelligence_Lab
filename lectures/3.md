# Lecture 3

22-11-30

## How can classification be done using the K-nearest neigbors algorithm?

Look at the k number of nearest neighbors and classify the new point based on the majority of the neighbors.

Minkowski distance, from sklearn

manhattan distance ????

choose k is important and find th through trial and error

it is depandennt on feture scaling

it is a fast algorithm

it can do classification but also regression by taking the mean of the k nearest neighbors

## What is a support vector machine?

try to find a line to separate the data (with the highest margen) in a 2d and (a plane in) 3d space (or a hyperplane in a higher dimension)

with overlapping data, u can use paramers to tund howmanny errors u exapt  (the c parameter in sklearn (higher c means less errors))

test wich c is best with trile and error

cross validation??

u can add extra demantions to the data to make it linearly seperable and then project it back to the original demantions (slide 16)

### svm in sklearn

we can use a pipeline to scale the data and then use the svm (SVC??)

## What is a decision tree algorithm and how can it be used for classification and regression?

(see slides 19-20)

in essence, a bunch of if this then that statements

try to optimize it so that if we split the data we get the best result

build a tree, start at th root and split one feature at a time

u can rotate youre data to make it linearly seperable (feature engineering, instead of using a different algorithm)

## What is purity and entropy in relation to a decision tree?

a decision tree is a bunch of if this then that statements and a nice way to visualize it is a tree

it is easy to overfit the data by havving to few instances in a leaf(node) (we can use more data or prune the tree)

so u can say, dont split less than 5 (n) instances

feature engineering: u can rotate the data to make it linearly seperable

a small difference in entropy can make a big difference in the result

### gini impurity

the probability of a random element being misclassified

see slice 22 for formula

max is 0.5

### entropy

the average information needed to classify an element

this takes more time but is more accurate (optimal) but gini is faster and also good

it uses logs and that is why it is slower

### regression trees

we draw a line instead of a split (slide 25)

we take the mean of the y values in the leaf to get the prediction

## What is a random forest algorithm?

we take multiple decision trees and combine (take the avarage) them to get a better result

a random forest is a bunch of decision trees build from differnt random subsets of the data

## What is bagging, and out of bag estimators

bagging is a way to get a better result by taking the avarage of multiple models

with disision trees, we call it a random forest

use the ensemble module in sklearn

## What are voting classifiers?

we dont do the random forest but we use just try different models and take the avarage of the results or the bets result

====================

## What are some of the challenges that are faced by ML system designers?

- not enough data
- data innconsistencies/poor quality
- nonrepresentative data
- irrelevant features

the training data should be representative of data u use in production

## Why do we need to split our data into different sets?

all data splits into training, validation and test data

70/30 or 80/20 train/test

than the training data is split into training and validation data

(maby retake a rendom set of validation data every iteration ??)

## What is the difference between a validation set and a test set?

validation set is used to train the model, test set is used to test the model and is never "seen" by the model

## How does a confusion matrix help to evaluate a binary classification system?

it shows the true positives, true negatives, false positives and false negatives

use the ax.grid() function to plot the grid

we use this becouse we might not caire about the false positives or false negatives

## What is TP, TN, FN and FP?

true positive: the model predicted a positive and it was positive

true negative: the model predicted a negative and it was negative

false positive: the model predicted a positive and it was negative

false negative: the model predicted a negative and it was positive

## What are the metrics accuracy, precision, recall, and f1_score, and what information can we obtain from them?

accuracy: the number of correct predictions divided by the total number of predictions $accuracy = \frac{TP + TN}{TP + TN + FP + FN}=$

accuracy is not always the best metric

precision: what proportion of positive identifications was actually correct $precision = \frac{TP}{TP + FP}$

recall: it you say all instances of a certain class are positive, what proportion of positive instances did you identify correctly? $recall = \frac{TP}{TP + FN}$

prisiion and recall are both important and if one is high the other is low (see slide 51)

a higher threshold will give a higher precision and a lower recall and vice versa

f1_score: $f1\_score = 2 \times \frac{precision \times recall}{precision + recall}$ (slide 52)

f1 score is the harmonic mean of precision and recall

selectivety takes the true negatives into account but is not talked about much

## What is the ROC-AUC and in which types of problems is it useful?

we plot the true positive rate (recall) against the false positive rate (1 - specificity) if we change the threshold

we want the area under the curve to be as big as possible

## What does a learning curve look like?

it is a performance measure of a model

epoch is the number of iterations

we plot error over the number of epochs

we find out if we are under or overfitting

the minimum error is the best model (slide 54)

## What are parameters and hyper-parameters?

parameters are the values that the model learns

hyperparameters are the values that we set before the model is trained ??

## Why would we need K-fold cross validation?

we split the data into k folds and then we train the model k times, each time using a different fold as the validation set and the rest as the training set, then we take the avarage of the results or the best result

## What is regularization?

if the slope of the line is big, it is bad

try to keep the higher order terms small

## What is meant by the terms batch and epoch

batch is the number of instances used in one iteration

epoch is the number of iterations

## Why would we want to do early stopping?

we dont want to overfit the data

we stop the training when the validation error starts to increase
