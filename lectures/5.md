# Lecture 5

22-12-14

## multilayer perceptron

a neural network with one or more hidden layers

## nural network hyperparameters

- number of hidden layers
- number of neurons in each layer
- batch size
- number of epochs
- activation function
- regularization/dropout
- learning rate
- batch normalization
- optimizer
- ...

## regularization

try to prevent overfitting by minimizing the higher order terms in the cost function

- dropout

every time you train a neural network, you randomly drop out some of the neurons in the hidden layers. just for that training step. this prevents the network from relying on any one neuron too much.

## vanishing gradient problem

when you have a deep neural network, the gradient can get very small. this makes it hard for the network to learn. there are a few ways to fix this:

- use a different activation function (tanh, relu, leaky relu), but the mpre complicated ones are more computationally expensive
- use batch normalization
- use residual networks ??
- improve weight initialization

## batch normalization

normalize the inputs to each layer. this makes the network more robust to different initializations. it also makes it easier to use a higher learning rate.

``` python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

But it has a cost so it may not be nessary if you have a small network.

## optimizer

- momentum optimization
- stochastic gradient descent (SGD)
- adaptive gradient optimization (AdaGrad)
- adam optimization

### momentum optimization

- keeps track of the direction of the previous gradients
- adds a fraction of the previous gradient to the current gradient
- this helps the algorithm to jump out of local minima

``` python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
```

### stochastic gradient descent

- instead of computing the gradients based on the full training set at every step, it computes the gradients based on a random subset of the training set
- this makes the algorithm much faster, but it also makes it a lot less regular
- this can help the algorithm jump out of local minima, but it can also prevent it from converging to the global minimum

``` python
optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-4)
```

### adaptive gradient optimization

- divides the learning rate by the square root of the sum of the squares of the gradients up to that point
- this makes the algorithm converge faster

``` python
optimizer = keras.optimizers.Adagrad(lr=0.001)
```

or

``` python
optimizer = keras.optimizers.rmsprop(lr=0.001, rho=0.9)
```

### adam optimization

- combines momentum optimization and adaptive gradient optimization
- it also keeps track of an exponential decaying average of past gradients

``` python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
```

## convolutional neural networks (CNN)

- convolutional filters
  - you slide a filter over the input image and compute the dot product of the filter and the input image at each location

### convolutional layers (stride)

- stride
  - the number of pixels you shift the filter at each step
  - if the stride is 1, you shift the filter one pixel at a time
  - if the stride is 2, you shift the filter two pixels at a time
  - if the stride is 3, you shift the filter three pixels at a time
  - etc.

### depth

- the number of filters in a convolutional layer
- the depth of the output feature map is equal to the number of filters in the convolutional layer

### padding

- valid padding
  - no padding
  - the output feature map is smaller than the input feature map
- same padding
    - the output feature map is the same size as the input feature map


### pooling layers

- max pooling
  - you slide a pooling filter over the input feature map and select the maximum value in the filter
- average pooling
    - you slide a pooling filter over the input feature map and compute the average value in the filter

q: why do we need pooling layers?
a: they reduce the size of the feature maps, which reduces the number of parameters in the network.

dont do this on non image data

### example

``` python
model = keras.models.Sequential([
    keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding="same", activation="relu", input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation="softmax")
])
```

## data augmentation

- flip the image horizontally
- shift the image up, down, left, or right
- rotate the image
- zoom in on the image
- shear the image

## leveraging pretrained models (transfer learning)

- you can use a pretrained model as the basis for your own model
- you can use the pretrained model as a fixed feature extractor

## project information

