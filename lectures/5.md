# Lecture 5

22-12-14

## multilayer perceptron

a neural network with one or more hidden layers

## nural network hyperparameters

- number of hidden layers
- number of neurons in each layer
- batch size
- number of epochs
- activation function
- regularization/dropout
- learning rate
- batch normalization
- optimizer
- ...

## regularization

try to prevent overfitting by minimizing the higher order terms in the cost function

- dropout

every time you train a neural network, you randomly drop out some of the neurons in the hidden layers. just for that training step. this prevents the network from relying on any one neuron too much.

## vanishing gradient problem

when you have a deep neural network, the gradient can get very small. this makes it hard for the network to learn. there are a few ways to fix this:

- use a different activation function (tanh, relu, leaky relu), but the mpre complicated ones are more computationally expensive
- use batch normalization
- use residual networks ??
- improve weight initialization

## batch normalization

normalize the inputs to each layer. this makes the network more robust to different initializations. it also makes it easier to use a higher learning rate.

``` python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

But it has a cost so it may not be nessary if you have a small network.

## optimizer

- momentum optimization
- stochastic gradient descent (SGD)
- adaptive gradient optimization (AdaGrad)
- adam optimization

### momentum optimization

- keeps track of the direction of the previous gradients
- adds a fraction of the previous gradient to the current gradient
- this helps the algorithm to jump out of local minima

``` python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
```

### stochastic gradient descent

- instead of computing the gradients based on the full training set at every step, it computes the gradients based on a random subset of the training set
- this makes the algorithm much faster, but it also makes it a lot less regular
- this can help the algorithm jump out of local minima, but it can also prevent it from converging to the global minimum

``` python
optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-4)
```

### adaptive gradient optimization

- divides the learning rate by the square root of the sum of the squares of the gradients up to that point
- this makes the algorithm converge faster

``` python
optimizer = keras.optimizers.Adagrad(lr=0.001)
```

or

``` python
optimizer = keras.optimizers.rmsprop(lr=0.001, rho=0.9)
```

### adam optimization

- combines momentum optimization and adaptive gradient optimization
- it also keeps track of an exponential decaying average of past gradients

``` python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
```

## convolutional neural networks (CNN)

- convolutional filters
  - you slide a filter over the input image and compute the dot product of the filter and the input image at each location

### convolutional layers (stride)

- stride
  - the number of pixels you shift the filter at each step
  - if the stride is 1, you shift the filter one pixel at a time
  - if the stride is 2, you shift the filter two pixels at a time
  - if the stride is 3, you shift the filter three pixels at a time
  - etc.

### depth

- the number of filters in a convolutional layer
- the depth of the output feature map is equal to the number of filters in the convolutional layer

### padding

- valid padding
  - no padding
  - the output feature map is smaller than the input feature map
- same padding
  - the output feature map is the same size as the input feature map

### pooling layers

- max pooling
  - you slide a pooling filter over the input feature map and select the maximum value in the filter
- average pooling
  - you slide a pooling filter over the input feature map and compute the average value in the filter

q: why do we need pooling layers?
a: they reduce the size of the feature maps, which reduces the number of parameters in the network.

dont do this on non image data

### example

``` python
model = keras.models.Sequential([
    keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding="same", activation="relu", input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding="same", activation="relu"),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation="softmax")
])
```

## data augmentation

- flip the image horizontally
- shift the image up, down, left, or right
- rotate the image
- zoom in on the image
- shear the image

## leveraging pretrained models (transfer learning)

- you can use a pretrained model as the basis for your own model
- you can use the pretrained model as a fixed feature extractor

## What is dropout, how does it work, what problem does it solve, and how is it implemented?

dropout is a regularization technique that randomly drops out some of the neurons in a layer during training. this forces the network to learn redundant representations, which makes it more robust. it also speeds up training.

``` python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
```

## What is the vanishing gradient problem and how can it be minimized?

the vanishing gradient problem is when the gradient of the cost function gets smaller and smaller as the algorithm progresses down to the lower layers. this slows down training. there are several solutions to this problem:

- use a different activation function
- use batch normalization
- use a different weight initialization technique
- use a different optimizer

## What is batch normalization?

batch normalization is a technique that makes the inputs to each layer have a mean of 0 and a standard deviation of 1. this makes it easier for the network to learn.

## What problem do convolution neural networks address?

convolution neural networks are good at detecting patterns in images. they are good at detecting edges, corners, and other patterns in images.

## What are some alternative optimizers to SGD, and what are their benefits and drawbacks?

- momentum optimization
  - it keeps track of an exponentially decaying average of past gradients
  - it makes the algorithm converge faster
  - it can overshoot the minimum

## What is a convolutional neural network, how does it work and what applications is it good for?

convolution neural networks are good at detecting patterns in images. they are good at detecting edges, corners, and other patterns in images.

## What are convolution kernels and filters?

convolution kernels are small matrices that are used to detect patterns in images. they are also called convolution filters.

## What does a convolutional layer look like and how does it work?

a convolutional layer consists of a stack of convolutional filters. you slide a filter over the input image and compute the dot product of the filter and the input image at each location. the output is called a feature map.

## What is meant by the terms padding and stride?

padding is the number of pixels you add to the input image. stride is the number of pixels you shift the filter at each step.

## How do you determine the image size at the output of a convolution layer?

the output image size is equal to the input image size divided by the stride. and you have to take padding into account.

## What is a pooling layer?

a pooling layer is a layer that reduces the size of the feature maps. it does this by sliding a pooling filter over the input feature map and computing the maximum or average value in the filter.

## What are some of the common CNN Architectures and what do they typically consist of?

??

## What is data augmentation and when is it used?

data augmentation is a technique that artificially increases the size of the training set by applying random transformations to the training images. this makes the model more robust and reduces overfitting.

## What is transfer learning and what are the benefits?

transfer learning is when you use a pretrained model as the basis for your own model. this can speed up training and make the model more accurate.

---

## Summary

### What is a convolutional neural network (CNN)?

A convolutional neural network (CNN) is a type of artificial neural network designed to process data with a grid-like topology, such as an image. It is made up of multiple layers of artificial neurons, including convolutional layers, pooling layers, and fully connected layers. In a CNN, the convolutional layers apply filters to the input data and create feature maps, which are then processed by the pooling layers and fully connected layers to produce output predictions. CNNs are commonly used for tasks such as image classification, object detection, and image generation.

### What are convolutional filters and convolutional layers in a CNN?

Convolutional filters are small matrix-like structures that are used to extract features from the input data in a CNN. Convolutional layers are made up of multiple convolutional filters that are applied to the input data. The filters are applied by sliding them over the input data and computing the dot product of the filter and the input data at each location. The resulting feature maps are then processed by the subsequent layers of the CNN to produce output predictions.

### What is stride in a convolutional layer?

Stride is the number of pixels that the convolutional filter is shifted at each step as it is applied to the input data in a convolutional layer. For example, if the stride is 1, the filter is shifted one pixel at a time; if the stride is 2, the filter is shifted two pixels at a time; and so on. The stride can be adjusted to control the size of the output feature maps and the amount of overlap between them.

### What is depth in a convolutional layer?

Depth in a convolutional layer refers to the number of filters in the layer. The depth of the output feature map is equal to the number of filters in the convolutional layer. Increasing the depth of the convolutional layer can allow the CNN to extract more features from the input data, but it can also increase the complexity and computational cost of the network.

### What are pooling layers in a CNN?

Pooling layers are used to down-sample the input data in a CNN. They are typically inserted between convolutional layers to reduce the size of the feature maps and control the complexity of the network. There are various types of pooling layers, including max pooling and average pooling. Max pooling selects the maximum value from each pooling region, while average pooling takes the average value. Pooling layers can help the CNN to be more robust to small translations and deformations in the input data.

### What are fully connected layers in a CNN?

Fully connected layers, also known as dense layers, are used in the final stage of a CNN to produce the final output predictions. They are called fully connected because each neuron in the layer is connected to every neuron in the previous layer. The output of the fully connected layer is a vector of probabilities that represents the likelihood of each possible class. Fully connected layers can be trained using techniques such as backpropagation and stochastic gradient descent.
