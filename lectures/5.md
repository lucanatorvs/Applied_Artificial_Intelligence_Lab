# Lecture 5

22-12-14

## multilayer perceptron

a neural network with one or more hidden layers

## nural network hyperparameters

- number of hidden layers
- number of neurons in each layer
- batch size
- number of epochs
- activation function
- regularization/dropout
- learning rate
- batch normalization
- optimizer
- ...

## regularization

try to prevent overfitting by minimizing the higher order terms in the cost function

- dropout

every time you train a neural network, you randomly drop out some of the neurons in the hidden layers. just for that training step. this prevents the network from relying on any one neuron too much.

## vanishing gradient problem

when you have a deep neural network, the gradient can get very small. this makes it hard for the network to learn. there are a few ways to fix this:

- use a different activation function (tanh, relu, leaky relu), but the mpre complicated ones are more computationally expensive
- use batch normalization
- use residual networks ??
- improve weight initialization

## batch normalization

normalize the inputs to each layer. this makes the network more robust to different initializations. it also makes it easier to use a higher learning rate.

``` python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

But it has a cost so it may not be nessary if you have a small network.

## optimizer

- momentum optimization
- stochastic gradient descent (SGD)
- adaptive gradient optimization (AdaGrad)
- adam optimization

### momentum optimization

- keeps track of the direction of the previous gradients
- adds a fraction of the previous gradient to the current gradient
- this helps the algorithm to jump out of local minima

``` python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
```

### stochastic gradient descent

- instead of computing the gradients based on the full training set at every step, it computes the gradients based on a random subset of the training set
- this makes the algorithm much faster, but it also makes it a lot less regular
- this can help the algorithm jump out of local minima, but it can also prevent it from converging to the global minimum

``` python
optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-4)
```

### adaptive gradient optimization

- divides the learning rate by the square root of the sum of the squares of the gradients up to that point
- this makes the algorithm converge faster

``` python
optimizer = keras.optimizers.Adagrad(lr=0.001)
```

or

``` python
optimizer = keras.optimizers.rmsprop(lr=0.001, rho=0.9)
```

### adam optimization

- combines momentum optimization and adaptive gradient optimization
- it also keeps track of an exponential decaying average of past gradients

``` python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
```

## convolutional neural networks
