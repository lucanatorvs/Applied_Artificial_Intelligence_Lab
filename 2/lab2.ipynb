{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Artificial Intelligence - Lab 2\n",
    "\n",
    "Luca van Straaten - 18073611\n",
    "\n",
    "## Preparation\n",
    "\n",
    "First I uninstalled anaconda by:\n",
    "- removing the lines from my .config/fish/config.fish\n",
    "- removing the path from fish\n",
    "- `brew uninstall anaconda`\n",
    "\n",
    "Than I installed miniconda by:\n",
    "\n",
    "```fish\n",
    "brew install miniconda\n",
    "```\n",
    "\n",
    "That was easy.\n",
    "\n",
    "uh, it did not install the newest version of conda, so I had to update it by:\n",
    "\n",
    "```fish\n",
    "conda update -n base -c defaults conda\n",
    "```\n",
    "\n",
    "**Maby make a pull request to the brew formula? - [if I ever feel like it]™**\n",
    "\n",
    "And I recreated the environment with python 3.10:\n",
    "\n",
    "```fish\n",
    "conda create -n aai_lab python=3.10\n",
    "conda activate aai_lab\n",
    "conda install tensorflow notebook pandas matplotlib numpy\n",
    "conda install scikit-learn\n",
    "conda install -c conda-forge nb_conda_kernels\n",
    "```\n",
    "\n",
    "**System information**: 2018 Intel Core i7 13-inch MacBookPro15,2, 16GB RAM, 512GB SSD, macOS Ventura 13.0 (22A380), kernel 22.1.0\n",
    "\n",
    "This file, along with the rest of the labs, are tracked in a git repository on github. [lab 2](https://github.com/lucanatorvs/Applied_Artificial_Intelligence_Lab/blob/main/2/lab2.ipynb)\n",
    "\n",
    "## Exercise 1 - A quick overview of machine learning project.\n",
    "\n",
    "The end goal of this lab is to train a linear regression model to make an insurance cost prediction. Write the necessary code and answer the questions below.\n",
    "\n",
    "### 1\n",
    "\n",
    "Use the CSV file from Blackboard Med_insurance.csv [2] from last week. It contains data of medical information and insurance cost. It contains 1338 rows of data with columns: age, gender, BMI, children, smoker, region, insurance charges. Read this csv file using pandas library into a variable called insurance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insurance_data = pd.read_csv(\"../Med_insurance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "Convert a categorical variable of your choice into dummy/indicator variables using the\n",
    "pandas function pandas.get_dummies() and combine the result with the numerical columns\n",
    "of the insurance_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = pd.get_dummies(insurance_data, columns=[\"smoker\"], drop_first=True)\n",
    "\n",
    "# now only keep the numeric columns\n",
    "insurance = insurance.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "Create a test set which is 20 % of the whole data set using a pure random sampling approach.\n",
    "\n",
    "**Question**: Why do you need a test set when training a model?\n",
    "\n",
    "**Answer**: To test the model on data it has not seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random as rnd\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    insurance, test_size=0.2, random_state=rnd.seed(42)\n",
    ")\n",
    "\n",
    "# Print the number of rows in the train and test set\n",
    "print(\"Number of rows in the train set: \", len(train_set))\n",
    "print(\"Number of rows in the test set: \", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Preparing the data for Machine Learning algorithms\n",
    "\n",
    "In the upcoming steps you will prepare the data that will be used to train a machine learning model\n",
    "\n",
    "### 1\n",
    "\n",
    "If you found missing values in the data add the missing entries for the respective column(s)\n",
    "using the imputer transform (including only numerical\n",
    "attributes) in Scikit SimpleImputer class. Use “median” as strategy. Make sure to train the imputer only on the training set.\n",
    "\n",
    "#### A\n",
    "\n",
    "Interpalat the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows for missing data in the train set and print them and save the index\n",
    "indexes = train_set[train_set.isnull().any(axis=1)].index\n",
    "print(train_set.loc[indexes])\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# fit the imputer to the train set\n",
    "imputer.fit(train_set)\n",
    "\n",
    "# transform the train set\n",
    "X = imputer.transform(train_set)\n",
    "\n",
    "# apply the imputer to the train set\n",
    "train_set_fixed = pd.DataFrame(X, columns=train_set.columns, index=train_set.index)\n",
    "\n",
    "print(train_set_fixed.loc[indexes])\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "train_set_fixed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B\n",
    "\n",
    "We also need to interpalate the test data:\n",
    "\n",
    "This is done completely seperate from the training data, so the test data is completely unseen by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows for missing data in the test_set and print them and save the index\n",
    "indexes = test_set[test_set.isnull().any(axis=1)].index\n",
    "print(test_set.loc[indexes])\n",
    "\n",
    "# fit the imputer to the test_set\n",
    "imputer.fit(test_set)\n",
    "\n",
    "# transform the test_set\n",
    "X = imputer.transform(test_set)\n",
    "\n",
    "# apply the imputer to the test_set\n",
    "test_set_fixed = pd.DataFrame(X, columns=test_set.columns, index=test_set.index)\n",
    "\n",
    "print(test_set_fixed.loc[indexes])\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "test_set_fixed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "Perform feature scaling on all numerical attributes\n",
    "using Scikit transform StandardScaler. Again, fit the scaler on the training data only.\n",
    "\n",
    "***Question***: Explain what problem or problems the feature scaling resolves.\n",
    "\n",
    "**Answer**: It makes the data more comparable, so that the model can learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature scaling on all numerical attributes\n",
    "# using Scikit transform StandardScaler. Again, fit the scaler on the training data only.\n",
    "# we will use the same scaler for the test set so we make a pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# apply the pipeline to the train set\n",
    "train_set_prepared = num_pipeline.fit_transform(train_set_fixed)\n",
    "\n",
    "# apply the pipeline to the test set\n",
    "test_set_prepared = num_pipeline.fit_transform(test_set_fixed)\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "print(train_set_prepared[:5])\n",
    "print(test_set_prepared[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prepocessed data into a variable `insurance_data_prepared`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_data_prepared = train_set_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Training a model\n",
    "\n",
    "In the following steps you will select and train a machine learning model.\n",
    "\n",
    "### 4\n",
    "\n",
    "Now your data is ready to be used in training a machine learning model. Use it to train a Linear Regression model that can predict insurance charges. See the Training and Evaluation on the Training Set section at page 72 in the book [1].\n",
    "\n",
    "**Question**: What is the difference between supervised and unsupervised learning? Is a Linear Regression supervised or unsupervised?\n",
    "\n",
    "**Answer**: Supervised learning is when you have a dataset with the correct answers, and unsupervised learning is when you don't. Linear Regression is supervised.\n",
    "\n",
    "**Question**: What is the difference between regression and classification?\n",
    "\n",
    "**Answer**: Regression is when you want to predict a continuous value, and classification is when you want to predict a discrete value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit the model to the train set\n",
    "lin_reg.fit(insurance_data_prepared, train_set[\"charges\"])\n",
    "\n",
    "# print the intercept and coefficients\n",
    "print(\"Intercept: \", lin_reg.intercept_)\n",
    "print(\"Coefficients: \", lin_reg.coef_)\n",
    "print(\"Number of coefficients: \", len(lin_reg.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "\n",
    "Test your trained model with the test data and find out the root mean squared error and\n",
    "mean absolute error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your trained model with the test data and find out the root mean squared error and\n",
    "# mean absolute error.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "insurance_data_predictions = lin_reg.predict(test_set_prepared)\n",
    "\n",
    "lin_mse = mean_squared_error(test_set[\"charges\"], insurance_data_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_mae = mean_absolute_error(test_set[\"charges\"], insurance_data_predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error: \", lin_rmse)\n",
    "print(\"Mean Absolute Error: \", lin_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('aai_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c148a4cd26e4f153191910e509663017fc090c88cc297ba3ef5842cf5140e93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
